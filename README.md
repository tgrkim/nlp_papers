# Pivotal NLP papers

This section traces the progression from advanced Recurrent Neural Network (RNN) models with attention to the complete dominance of the Transformer architecture, which laid the groundwork for modern Large Language Models (LLMs).

* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025) (Luong, et al., 2015) - A foundational paper that explores and simplifies different attention mechanisms for sequence-to-sequence models, providing a clear starting point for implementing attention on top of RNNs.
[(Code Repo)](https://github.com/lmthang/nmt.hybrid)

* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) (Vaswani, et al., 2017) - The revolutionary paper that introduced the Transformer architecture, abandoning recurrence entirely in favor of self-attention. This is the essential starting point for understanding modern LLMs [(Code Repo)](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805) (Devlin, et al., 2018) - Introduced the concept of pre-training a deep bidirectional Transformer using a Masked Language Model (MLM) objective, fundamentally changing the paradigm for transfer learning in NLP. [(Code Repo)](https://github.com/google-research/bert)

* [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) (Radford, et al., 2018) - This is the original GPT paper. It established the power of the "generative pre-training" followed by "discriminative fine-tuning" approach for a decoder-only Transformer, setting the stage for large-scale language generation. [(Code Repo)](https://github.com/openai/finetune-transformer-lm)

* [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461) (Lewis, et al., 2019) - Introduced a pre-training scheme that corrupts text with an arbitrary noising function and learns a model to reconstruct the original. This combination of a bidirectional encoder (like BERT) and an autoregressive decoder (like GPT) proved extremely effective for both NLU and NLG tasks. [(Code Repo)](https://github.com/facebookresearch/fairseq/tree/main/examples/bart)

* [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (Radford, et al., 2019) & [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165) (Brown, et al., 2020) - These are the GPT-2 and GPT-3 papers, respectively. GPT-2 demonstrated that a scaled-up language model could perform tasks zero-shot without explicit training. GPT-3 massively scaled this principle, revealing the emergent ability of "in-context learning" (few-shot), which became a new paradigm for interacting with models. ([GPT-2 Code](https://github.com/openai/gpt-2), GPT-3 was API-only)

* [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/pdf/2109.01652) (Wei, et al., 2021) - This paper (introducing FLAN) is a key player in Supervised Fine-Tuning (SFT). It showed that fine-tuning a large language model on a massive collection of diverse tasks described via natural language instructions dramatically improves its zero-shot performance on unseen tasks, making models more general and usable.

* [Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/pdf/2203.02155) (Ouyang, et al., 2022) - This is the seminal "InstructGPT" paper that detailed the Reinforcement Learning from Human Feedback (RLHF) process. It established the three-step method (SFT, Reward Modeling, RL Optimization) for aligning models with user intent, making them more helpful, honest, and harmless. This is the core technique behind ChatGPT.

* [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961) (Fedus, et al., 2021) - A key paper for Mixture of Experts (MoE), it successfully implemented a sparsely activated model at an unprecedented scale (trillions of parameters). It demonstrated that using a router to send each token to a specific "expert" network allows for a massive increase in model size with a near-constant computational cost, paving the way for more efficient large models.

* [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/pdf/2111.00396) (Gu, et al., 2021) - This paper, introducing S4, is a foundational work for modern State Space Models (SSMs). It connects the theory of SSMs from control theory to deep learning, showing how they can be formulated to efficiently model very long-range dependencies in a way that is much faster than Transformers, both in training (parallel) and inference (recurrent).

* [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752) (Gu & Dao, 2023) - Building on the SSM foundation, Mamba introduced a key innovation: a selection mechanism that allows the model's parameters to be input-dependent. This gives SSMs the ability to focus on or ignore parts of the input selectively, a crucial capability that was previously a hallmark of attention mechanisms, leading to remarkable performance on various sequence modeling tasks.
